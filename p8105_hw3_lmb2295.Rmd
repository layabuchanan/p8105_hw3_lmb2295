---
title: "Homework 3"
author: "Laya Buchanan"

date: 2020-09-21
output: github_document
---

This is my submission for the third homework assignment for P8105.  

```{r message = FALSE, echo = FALSE}
library(tidyverse)
library(knitr)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1: Instacart Data

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and ... columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```


Let's make a plot

```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


Let's make a table!!

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```


Apples vs ice cream..

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```

# Problem 2: Accelerometer Data

## Read in the data

Read in the Accelerometer dataset. 

```{r message = FALSE}
accel_df = read_csv("./data/accel_data.csv")
```

## Clean the data

This code cleans up column names and creates a weekday vs weekend variable. 
```{r}
accel_df =
  janitor::clean_names(accel_df) %>%  
  mutate(
    weekend = case_when(
      day == "Monday" ~ "no",
      day == "Tuesday" ~ "no",
      day == "Wednesday" ~ "no",
      day == "Thursday" ~ "no",
      day == "Friday" ~ "no",
      day == "Saturday" ~ "yes",
      day == "Sunday" ~ "yes",
      TRUE ~ ""
    )) %>%
  mutate(weekend = recode(weekend, "yes" = TRUE, "no" = FALSE)) %>% 
  relocate(weekend, .after = day)
  
    
```


Next, I will check to see if the data is encoded with reasonable variable classes.

```{r}
accel_class_df = 
  accel_df %>% 
  summarise_all(typeof) %>% 
  select(week, day_id, day, weekend, activity_1)

kable(accel_class_df)
```

```{r echo = FALSE}
remove(accel_class_df)
```

We can see from the table above that week, day_id, and the activity_* variables are numeric, the weekend variable is logical, and the day variable is a character variable. These are reasonable classes for the variable types so no further action is needed.

The resulting dataset contains 35 observations and 1444 variables. The 35 observations represent 35 days of accelerometer data collected on a 63 year-old male with a BMI of 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure. The variables include the week number for each of the seven weeks recorded, the number of the day recorded, the day of the week for the observed day, a variable stating whether or not the observed day was a weekend day, as well as 1440 activity_* variables, each representing an activity count for each minute in the day.

## Aggregate data

This code creates an aggregate variable displaying total activity each day.
```{r}
accel_df = 
  accel_df %>% 
  rowwise() %>% 
  mutate(total_activity = sum(c_across(activity_1:activity_1440)))%>% 
  relocate(total_activity, .after = weekend)
```


In this next chunk of code, I am using a table to explore whether there is any obvious relationship between total daily activity and the day of monitoring or day of the week.
```{r}
aggregate_df = 
  accel_df %>% 
  select(day_id, day, total_activity)

kable(aggregate_df)
```

There doesn't seem to be any obvious patterns. How about a chart?


```{r}
accel_df %>% 
  ggplot(aes(x = day_id, y = total_activity)) + 
  geom_point(aes(size = 4, color = day), alpha = .7) + geom_line(aes(), alpha = .5)+ 
  labs(
    title = "Day of Measurement and Total Daily Activity",
    x = "Day of Measurement",
    y = "Total Activity Count for Day",
    caption = "Activity counts displayed are in the hundered thousands, ex: 2 = 200000")+
  scale_y_continuous(
    breaks = c(200000, 400000, 600000), 
    labels = c(2, 4, 6))

```


We can kind of see some patterns here. This man's activity level is very variable on the weekends, with both some of his highest and loWest activity days occuring on the weekends. His activity levels remain closer to his mean in the middle of the week. Here's a chart to more fully explore this pattern:


```{r}
accel_df %>% 
  ggplot(aes(x = weekend, y = total_activity)) + 
  geom_boxplot(aes(), alpha = .5) + 
  labs(
    title = "Weekend and Total Daily Activity",
    x = "Weekend",
    y = "Total Activity Count for Day",
    caption = "Activity counts displayed are in the hundered thousands, ex: 2 = 200000") +
  scale_y_continuous(
    breaks = c(200000, 400000, 600000), 
    labels = c(2, 4, 6))
```

This chart confirms our previous observations. While the mean total daily activity is pretty similar whether or not it's the weekend, it is a little lower on the weekend with a much larger range of values.


What about in the course of the day?
```{r}
accel_df %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_number",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  ggplot(aes(x = activity_number, y = activity_count, color = day)) +
  geom_line() + 
  labs(
    title = "24 hour Activity Time Courses",
    x = "Hour",
    y = "Activity Count") +
  scale_x_discrete(
    breaks = c(180, 360, 540, 720, 900, 1080, 1260), 
    labels = c("3am", "6am", "9am", "12pm", "3pm", "6pm", "9pm" ))
```

We can see from this chart that this man is most active in the evening hours, the early morning hours, and sometimes midday. 


# Problem 3: NY NOAA Data

## Read in the data


```{r message = FALSE, results = "hide"}
library(p8105.datasets)
data("ny_noaa")
```

This dataset is a table consisting of 259176 observations and 7 variables. These variables include id (the weather station ID), date, prcp (precipitation), snow and snwd (snowfall and snowdepth in mm), tmax and tmin (the maximum and minimum temperature recorded in tenths of degrees C). A considerable number of measurements for the precipitation, snowfall, and temperature measurements are missing, which could present issues depending on the data analysis you intend to conduct.

## Data Cleaning

This code creates separate variables for year, month and day, lists the name of the month rather than the number, and removes rows with no measurements for precipitation, snow, or temperature.

Now, I will check to see if the variables are encoded with reasonable class types:
```{r}
noaa_class_df = 
  ny_noaa %>% 
  summarise_all(typeof)

kable(noaa_class_df)
```

```{r echo = FALSE}
remove(noaa_class_df)
```

tmin and tmax currently exists as character vectors. This could present a problem for data analysis. I will address this issue in the next cleaning step.


Here, I am separating the data variable into month, day, and year variables, converting the minimum and maximum temperatures into a more reasonable class type (numeric), and changing the snow and temperature units into more intuitive units (cm for precipitation and snow measurements and degrees Celsius for temperature measurements).

```{r}
ny_noaa_df = 
ny_noaa %>% 
  separate(date, into = c("year", "month", "day")) %>% 
  mutate(month = recode(month, `01` = "january", `02` = "february",  `03` = "march", `04` = "april", `05` = "may", `06` = "june", `07` = "july", `08` = "august", `09` = "september", `10` = "october", `11` = "november", `12` = "december")) %>% 
  mutate(
    tmin = as.numeric(tmin),
    tmax = as.numeric(tmax),
    pcrp = prcp/100,
    tmin = tmin/10,
    tmax = tmax/10,
    snow = snow/10,
    snow = snow/10
  )

  
```

This data has been cleaned, but we can see that there are some datapoints with unreasonable values. For example, in August and September 2008 where nearly all of the temperature measurements from the station USR0000NGAN are 60 degrees. These are clearly unreasonably high temperatures for NY, even in August and it is reasonable to assume that the measurements taken by this station during this time period were taken in error.

```{r}
ny_noaa %>% 
  group_by(snow) %>%
  summarize(
    n_obs = n())
```

The most commonly observed snowfall value is 0cm because it does not snow on most days in New York. We can also see that on one day, 0.13cm of snowfall was recorded in error, another clear measurement error.

```{r warning = FALSE}
jan_p = 
ny_noaa_df %>% 
  filter(month == "january") %>%
  group_by(id, year) %>% 
  mutate(id_tmax_mean = mean(tmax, na.rm = TRUE)) %>% 
  group_by(year) %>% 
  ggplot(aes(x = year, y = id_tmax_mean)) + 
    geom_boxplot() + geom_line() +
  scale_x_discrete(
    breaks = c(1985, 1990, 1995, 2000, 2005)) +
  labs(
    title = "Average Maximum Temperature in January",
    x = "Year",
    y = "Average Maximum Temperature")


  
jul_p = 
ny_noaa_df %>% 
  filter(month == "july") %>%
  group_by(id, year) %>% 
  mutate(id_tmax_mean = mean(tmax, na.rm = TRUE)) %>% 
  group_by(year) %>% 
  ggplot(aes(x = year, y = id_tmax_mean)) + 
    geom_boxplot() + geom_line() +
  scale_x_discrete(
    breaks = c(1985, 1990, 1995, 2000, 2005)) +
  labs(
    title = "Average Maximum Temperature in July",
    x = "",
    y = " (Degrees Celcius)")

jul_p/jan_p

```
This is a a two-panel plot showing the average max temperature in January and in July in each station across years.There doesn't appear to be much of a pattern in either chart, although maximum temperatures in July seem to be much more stable than those in January. There are some outliers in the data, the most notable being the low measurement in July 1988. 

```{r warning = FALSE}
tmaxmin_p = 
  ggplot(ny_noaa_df, aes(x = tmax, y = tmin)) + 
  geom_bin2d() +
labs(
    title = "Max daily Temperature Frequencies",
    x = "Maximum Temperature (C)",
    y = "Minimum Temperature (C)") +
  theme(legend.position = "right")

snow_p = 
  ny_noaa_df %>% 
  filter(snow > 0, snow < 101.60) %>% 
  group_by(year) %>% 
  ggplot(aes(x = year, y = snow, color = "blue")) + 
  geom_point(alpha = .15) +
    scale_x_discrete(
    breaks = c(1985, 1990, 1995, 2000, 2005)) +
  labs(
    title = "Snowfall Events Volume by Year",
    x = "Year",
    y = "Snowfall (cm)" 
  ) +
    theme(legend.position = "none")

tmaxmin_p+snow_p
```

This is a two panel chart with the first panel showing the minimum vs maximum temperature for the full dataset. Most of the measurements for both the minimum and the maximum are clustered around the same values, between -30 and 30 degrees. There are some outliers going as high as 60 degrees and as low as nearly -60 degrees, again, this is likely due to measurement errors discussed above. The second panel is a plot make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year. You can see that overwhelmingly most of the snowfall events had a volume of less than 15 cm of snowfall. However, there are some extreme outliers up to nearly 80 cm of snow in the 1980s. 